<!DOCTYPE html>
<html>
   <head>
      <meta charset="utf-8" />
      <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
      <!-- Replace the content tag with appropriate information -->
      <meta
         name="description"
         content="Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning"
         />
      <meta
         property="og:title"
         content="Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning"
         />
      <meta
         property="og:description"
         content="Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning"
         />
      <meta
         property="og:url"
         content="https://berkegokmen1.github.io/dual-enc-3d-gan-inv/"
         />
      <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
      <meta property="og:image" content="static/images/mesh.png" />
      <meta property="og:image:width" content="856" />
      <meta property="og:image:height" content="856" />
      <meta
         name="twitter:title"
         content="Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning"
         />
      <meta
         name="twitter:description"
         content="Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning"
         />
      <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
      <meta name="twitter:image" content="static/images/mesh.png" />
      <meta name="twitter:card" content="static/images/mesh.png" />
      <!-- Keywords for your paper to be indexed by-->
      <meta
         name="keywords"
         content="Vision-and-Language Navigation, Large Language Model, Unmanned Aerial Vehicles, Emboided, Navigation, Top-down map, Zero-shot, Action Prediction"
         />
      <meta name="viewport" content="width=device-width, initial-scale=1" />
      <title>STMR VLN</title>
      <link rel="icon" type="image/x-icon" href="static/favicon/favicon.ico" />
      <link
         href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
         rel="stylesheet"
         />
      <link rel="stylesheet" href="static/css/bulma.min.css" />
      <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
      <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
      <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
      <link
         rel="stylesheet"
         href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
         />
      <link rel="stylesheet" href="static/css/index.css" />
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
      <script defer src="static/js/fontawesome.all.min.js"></script>
      <script src="static/js/bulma-carousel.min.js"></script>
      <script src="static/js/bulma-slider.min.js"></script>
      <script src="static/js/index.js"></script>
   </head>
   <body>
      <section class="hero">
         <div class="hero-body">
            <div class="container is-max-desktop">
               <div class="columns is-centered">
                  <div class="column has-text-centered">
                     <h1 class="title is-1 publication-title">
                        Aerial Vision-and-Language Navigation via 
                        Semantic-Topo-Metric Representation Guided LLM Reasoning
                     </h1>
                     <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                        <a
                           >Yunpeng Gao</a
                           ><sup>1,2*</sup>,</span
                           >
                        <span class="author-block">
                           Zhigang Wang</a
                           ><sup>2*</sup>,</span
                           >
                        <span class="author-block">
                        <a
                           >Linglin Jing</a
                           ><sup>2</sup>,
                        </span>
                        <span class="author-block">
                        <a
                           >Dong Wang</a
                           ><sup>2</sup>,
                        </span>
                        <span class="author-block">
                        <a
                           >Xuelong Li</a
                           ><sup>2,3</sup>,
                        </span>
                        <span class="author-block">
                        <a
                           >Bin Zhao</a
                           ><sup>1,2†</sup>
                        </span>
                     </div>
                     <div class="is-size-5 publication-authors">
                        <span class="author-block">Northwestern Polytechnical University,</span>
                        <span class="author-block">Shanghai AI Laboratory,</span>
                        <span class="author-block">Institute of Artificial Intelligence, China Telecom Corp Ltd</span>
                        <span class="eql-cntrb"
                           ><small
                           ><br /><sup>*</sup>Indicates Equal Contribution</small
                           ></span
                           >
                        <span class="eql-cntrb"
                           ><small
                           ><br /><sup>†</sup>Indicates Corresponding Author</small
                           ></span
                           >
                     </div>
                     <div class="column has-text-centered">
                        <div class="publication-links">
                           <!-- Arxiv PDF link -->
                           <span class="link-block">
                           <a
                              href="https://arxiv.org/pdf/2410.08500"
                              target="_blank"
                              class="external-link button is-normal is-rounded is-dark"
                              >
                           <span class="icon">
                           <i class="fas fa-file-pdf"></i>
                           </span>
                           <span>Paper</span>
                           </a>
                           </span>
                           <!-- Github link -->
                           <span class="link-block">
                           <a
                              href="https://eziotic.github.io/STMR-VLN.github.io/"
                              target="_blank"
                              class="external-link button is-normal is-rounded is-dark"
                              >
                           <span class="icon">
                           <i class="fab fa-github"></i>
                           </span>
                           <span>Code</span>
                           </a>
                           </span>
                           <!-- ArXiv abstract Link -->
                           <span class="link-block">
                           <a
                              href="https://arxiv.org/pdf/2410.08500"
                              target="_blank"
                              class="external-link button is-normal is-rounded is-dark"
                              >
                           <span class="icon">
                           <i class="ai ai-arxiv"></i>
                           </span>
                           <span>arXiv</span>
                           </a>
                           </span>
                        </div>
                     </div>
                  </div>
               </div>
            </div>
         </div>
      </section>
      <!-- End video carousel -->
      <!-- Paper abstract -->
      <section class="section hero is-light">
         <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
               <div class="column is-four-fifths">
                  <h2 class="title is-3">Abstract</h2>
                  <div class="content has-text-justified">
                     <p>
                        Aerial Vision-and-Language Navigation (VLN) is a novel task
                        enabling Unmanned Aerial Vehicles (UAVs) to navigate in
                        outdoor environments through natural language instructions
                        and visual cues. It remains challenging due to the complex
                        spatial relationships in outdoor aerial scenes. In this
                        paper, we propose an end-to-end zero-shot framework for
                        aerial VLN tasks, where the large language model (LLM) is
                        introduced as our agent for action prediction.
                        Specifically, we develop a novel Semantic-Topo-Metric
                        Representation (STMR) to enhance the spatial reasoning
                        ability of LLMs. This is achieved by extracting and
                        projecting instruction-related semantic masks of landmarks
                        into a top-down map that contains the location information
                        of surrounding landmarks. Further, this map is transformed
                        into a matrix representation with distance metrics as the
                        text prompt to the LLM, for action prediction according to
                        the instruction. Experiments conducted in real and
                        simulation environments have successfully proved the
                        effectiveness and robustness of our method, achieving 15.9%
                        and 12.5% improvements (absolute) in Oracle Success Rate
                        (OSR) on AerialVLN-S dataset. 
                     </p>
                  </div>
               </div>
            </div>
         </div>
      </section>
      
      <!-- End paper abstract -->
      <!-- Paper content -->
      <section class="hero is-small">
         <div class="hero-body">
         <div class="container">
         <h2 class="title is-3">Method</h2>
         <p class="justified-text">
         In this paper, we introduce a novel zero-shot framework that leverages large language models (LLMs) for action prediction in aerial VLN tasks.
         </p>
         <br><br><br>
         <div class="item item-video1">
            <img src="static/images/overview.png" width="75%" class="center" alt="Training" />                  
            <h2 class="has-text-centered">
               <br>
               <p class="justified-text">
               Our method consists of three modules, i.e. sub-goal extracting, Semantic-Topo-Metric Representating, and LLM planner. First, we prompt an LLM to break down long instructions and extract specific landmarks, which will be classified by perception models, generating corresponding semantic masks. Then we project these masks into a top-down map using the UAV-view pose and depth information. The map is further compressed into a matrix representation. Finally, we prompt LLM with well-designed instructions to enable effective reasoning.
               </p>
               <br>
            </h2>
         </div>
         <div class="item item-video1">
            <img src="static/images/STMR.png" alt="Inference" />                  
            <h2 class="has-text-centered justified-text">
               <br>
               Firstly, we extract instruction-related landmarks and obtain corresponding semantic masks by perception models. After that, semantic masks obtained from each step are gradually projected into a top-down map using depth and pose transformation. To integrate rich visual information and topology into the text prompt while maintaining its simplicity, we separate the top-down map centered at the UAV's current position into grids and substitute each grid with a semantic number. This process transforms the top-down map into a matrix representation to serve as a spatial prompt for the LLM.
            </h2>
         </div>
      </section>
      <!-- Youtube video -->
      <section class="hero is-small is-light">
         <div class="hero-body">
         <div class="container">
         <h2 class="title is-3">Demo Video</h2>
         <div class="item item-video1">
           <br>
           <center>
             <iframe width="640" height="360" src="https://www.youtube.com/embed/QCurAuA0NXg" frameborder="0"
               allowfullscreen></iframe>
             </video>
           </center>
         </div>
         </div>
      </section>
      
      <!-- Paper poster -->
      <section class="hero is-small">
         <div class="hero-body">
         <div class="container">
         <h2 class="title is-3">Qualitative result</h2>
         <div class="item item-video1">
            <img src="static/images/result.png" width="75%" class="center" alt="Training" />
            <h2 class="subtitle has-text-centered justified-text">
               <br>
               Qualitative result of our method in the real-world scenarios.
               <p style="font-size: small;">Our method can align visual and textual landmarks as well as understand commands. Finally, the UAV reaches the destination successfully.</p>

            </h2>
         </div>
      </section>
      <!--End paper poster -->
      <!--BibTex citation -->
      <section class="section" id="BibTeX">
         <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{gao2024stmraerialvln,
                              title={Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning}, 
                              author={Yunpeng Gao and Zhigang Wang and Linglin Jing and Dong Wang and Xuelong Li and Bin Zhao},
                              year={2024},
                              eprint={2410.08500},
                              archivePrefix={arXiv},
                              primaryClass={cs.RO},
                              url={https://arxiv.org/abs/2410.08500}, 
                        }</code></pre>
         </div>
      </section>
      <!--End BibTex citation -->
      <footer class="footer">
         <div class="container">
            <div class="columns is-centered">
               <div class="column is-8">
                  <div class="content">
                     <p>
                        This page was built using the
                        <a
                           href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                           target="_blank"
                           >Academic Project Page Template</a
                           >
                        which was adopted from the <a
                           href="https://nerfies.github.io"
                           target="_blank"
                           >Nerfies</a
                           > project page. <br />
                        This website is licensed under a
                        <a
                           rel="license"
                           href="http://creativecommons.org/licenses/by-sa/4.0/"
                           target="_blank"
                           >Creative Commons Attribution-ShareAlike 4.0 International
                        License</a
                           >.
                     </p>
                  </div>
               </div>
            </div>
         </div>
      </footer>
      <!-- Statcounter tracking code -->
      <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
      <!-- End of Statcounter Code -->
   </body>
</html>
